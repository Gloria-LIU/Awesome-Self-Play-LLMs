# Awesome-Self-Play-LLMs
A collection of AWESOME things about Finetuning LLMs with Self-Play Mechanisms.

Fine-tuning Large Language Models (LLMs) for downstream tasks typically requires vast amounts of human-annotated data. However, self-training and self-improvement methods, which could reduce reliance on additional human annotations beyond the fine-tuning dataset, remain relatively underexplored. This repository addresses that gap by curating research papers that investigate how self-play techniques can enhance LLM reasoning and alignment.

## Related Works
- (*Arxiv 2024.11*) Self-Evolved Reward Learning for LLMs [[paper](https://arxiv.org/abs/2411.00418)]
- (*Arxiv 2024.11*) Self-Generated Critiques Boost Reward Modeling for Language Models [[Paper](https://arxiv.org/abs/2411.16646)]
- (*Arxiv 2024.08*) Self-Taught Evaluators [[paper](https://arxiv.org/abs/2408.02666)]
- (*Arxiv 2024.07*) Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge [[paper](https://arxiv.org/abs/2407.19594)]
- (*NeurIPS 24'*) Self-playing Adversarial Language Game Enhances LLM Reasoning [[paper](https://arxiv.org/abs/2404.10642)] [[code](https://github.com/Linear95/SPAG)]
- (*NeurIPS 24'*) Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing [[paper](https://arxiv.org/abs/2404.12253)]
- (*Arxiv 2024.01*) Self-Rewarding Language Models [[paper](https://arxiv.org/abs/2401.10020)]
- (*ICML 24'*) Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models [[paper](https://arxiv.org/abs/2401.01335)] [[code](https://github.com/uclaml/SPIN)]
- (*ACL 24' findings*) Adversarial Preference Optimization: Enhancing Your Alignment via RM-LLM Game [[paper](https://arxiv.org/abs/2311.08045)] [[code](https://github.com/Linear95/APO)]


## Contributing
üëç Contributions to this repository are welcome! 

If you have come across relevant resources, please feel free to open an issue or submit a pull request.

```
- (*conference|journal*) paper_name [[pdf](link)][[code](link)]
```
