# Awesome-Self-Play-LLMs
A collection of AWESOME things about Finetuning LLMs with Self-Play Mechanisms.


## Related Works
- (*Arxiv 2024.11*) Self-Evolved Reward Learning for LLMs [[paper](https://arxiv.org/abs/2411.00418)]
- (*Arxiv 2024.11*) Self-Generated Critiques Boost Reward Modeling for Language Models [[Paper](https://arxiv.org/abs/2411.16646)]
- (*Arxiv 2024.08*) Self-Taught Evaluators [[paper](https://arxiv.org/abs/2408.02666)]
- (*Arxiv 2024.07*) Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge [[paper](https://arxiv.org/abs/2407.19594)]
- (*NeurIPS 24'*) Self-playing Adversarial Language Game Enhances LLM Reasoning [[paper](https://arxiv.org/abs/2404.10642)]
- (*NeurIPS 24'*) Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing [[paper](https://arxiv.org/abs/2404.12253)]
- (*Arxiv 2024.01*) Self-Rewarding Language Models [[paper](https://arxiv.org/abs/2401.10020)]
- (*ICML 24'*) Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models [[paper](https://arxiv.org/abs/2401.01335)]
- (*ACL 24' findings*) Adversarial Preference Optimization: Enhancing Your Alignment via RM-LLM Game [[paper](https://arxiv.org/abs/2311.08045)]


## Contributing
üëç Contributions to this repository are welcome! 

If you have come across relevant resources, please feel free to open an issue or submit a pull request.

```
- (*conference|journal*) paper_name [[pdf](link)][[code](link)]
```
